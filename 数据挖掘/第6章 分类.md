# 训练集和测试集

### 1.留出法

直接将数据集划分为两个互斥集合，其中一个集合作为训练集，留下的集合作为测试集

 常见的做法是将2/3~4/5的样本作为训练集，剩下的作为测试集

### 2.交叉验证法

将源数据集划分为大小相似的若干互斥子集，然后用每组子集数据分别作为一次测试集，剩下k-1组子集数据作为训练集，得到k个模型，用这k个模型分类准确率的平均数作为此次交叉验证法的性能指标

###  3.自助法

适用于样本量较小的情况，原理是从n个样本的数据集中，有放回地随机抽取m个样本作为新的数据集

# 分类模型

### KNN

KNN是一种基于类比学习的分类算法，其原理是在训练集中找出k个与预测样本距离最近且最相似的样本，这些样本大部分属于哪个类别，则该预测样本也属于哪个类别

**优点**

1. 简单好用，容易理解，精度高，理论成熟，既可以用于分类，也可以用于回归
2. 可用于数值型数据和离散型数据
3. 训练时间复杂度为O(n)，无数据输入假定
4. 对异常值不敏感

**缺点**

1. 计算复杂性高、空间复杂性高
2. 当样本不平衡时容易误分
3. 一般样本数量很大时不用KNN，因为计算量很大，但数据样本量又不能太少，否则容易误分
4. 无法给出数据的内在含义

通过交叉验证来确定K值

### Rocchio

广泛应用于信息检索、文本分类、查询扩展等领域

**优点**

- 容易理解和实现，训练和分类计算特别简单，常用来实现衡量分类系统性能的基准系统

**缺点**

该算法源于两个假设

- 一是假设同类别的文档仅聚集在一个质心的周围，但对于线性不可分的文档集失效
- 二是假设训练数据是绝对正确的，但对于含有噪声的数据，该算法效果较差

### 决策树分类模型

决策树算法核心的问题是**分裂属性的选取**和**决策树的剪枝**

#### ID3

ID3算法的核心思想就是以信息增益来度量属性的选择，选择信息增益最大的属性进行分裂

特征A的信息增益等于数据集D的熵减去特征A对于数据集D的条件熵

Gain(D,A)=H(D)-H(D|A)

**优点**

1. 该算法使用全部的训练数据，充分利用全部训练样本的统计性质进行决策，从而抵抗噪声
2. 算法采用自顶向下的策略，搜索全部空间的一部分，保证所做的测试次数最少，分类速度快，其计算时间是样本个数、属性个数和结点个数的线性函数
3. 算法思路清晰，且用信息论作为基础

**缺点**

1. 采用互信息的计算方法会有多值偏向问题，即偏向于属性取值较多的测试属性，但取值较多的属性并不一定代表是最优属性
2. ID3算法是一种自顶向下的贪心算法，如果是非增量学习任务，此算法常常是建立决策树的最佳选择；对于增量学习任务，每次样本增加必须重新构建决策树，必然造成很大的开销
3. 对噪声特别敏感
4. 每个节点只含一个测试属性，是一种单变量算法，假设属性之间不存在相关性，虽然把多个属性用一棵树连接在一起，但这种关系是松散的，由于是单变量算法，表达复杂概念时非常困难
5. 无法处理某个属性取值缺少的问题，构造出的决策树不够完整
6. 只能对离散变量进行处理

### C4.5

是ID3算法的一种延伸和优化，克服了ID3算法的多值偏向问题，对树的剪枝也有较成熟的方法

C4.5引进**信息增益率**(最大的作为分裂属性)选择分裂节点：定义为信息增益于训练数据集关于特征A的熵之比

G_R(D,A)=Gain(D,A)/Ha(D)

#### C4.5算法的剪枝

在决策树的构建过程中，因为数据中噪声和孤立点的影响，部分分支反映出训练集中的一场，因为必须通过剪枝处理

1. 预剪枝法：通过提前停止树的构造而实现对数值的修建，一旦停止，结点成为树叶
2. 后剪枝法：被公认比较合理，在树构建之后才进行剪枝。通过对分枝结点删除，减去树结点，比较常用的是代价复杂性剪枝算法

**优点**

1. 用信息增益率来选择分裂属性，克服了用信息增益选择属性时偏向选择多属性的不足
2. 在树构造过程中进行剪枝
3. 能够完成对连续属性的离散化处理
4. 能够对不完整数据进行处理

**缺点**

主要缺点是在构造树的过程中需要对数据集进行多次顺序扫描和排序，因而导致算法低效

### CART

既可以用于分类也可以用于回归

永远是二叉，使用基尼系数（最小）来选择最好的数据分割特征，同时决定该特征的最优二值切分点

**优点**

1. 生成可以理解的规则
2. 计算量相对来说较小
3. 可以处理值为连续型和离散型的字段
4. 决策树可以清晰地显示哪些字段比较重要

**缺点**

1. 对连续性的字段比较难预测
2. 对于有时间顺序的数据，预处理工作比较复杂
3. 当类别太多时，错误可能会增加得比较快



小样本选择C4.5，大样本选择CART



### 贝叶斯分类

贝叶斯分类是一种最常用的**有指导学习**的方法，是以**贝叶斯定理**为理论基础的一种在已知**先验概率与条件概率**情况下得到**后验概率**的**模式识别**方法

#### 朴素贝叶斯分类器

贝叶斯定理：P(A|B)=P(AB)/P(B)=P(B|A)*P(A)/P(B) 

P(A|B)表示B发生的条件下A发生的概率(后验概率)



# 性能评估

#### 准确率和错误率

accuracy=(TP+TN)/(P+N)

#### 敏感性和特效性

敏感性又称真正例率，表示了分类器所识别出的正例占所有正例的比例。

sensitivity=TP/P

特效性又称真负例率：TN/N

#### 精确率与召回率

精确率表示 **模型预测为正例的样本中，实际确实是正例的比例**，又被称为查准率

precision=(TP)/(TP+FP)

召回率表示 **模型正确识别出的正样本数量占所有真实正样本数量的比例**，又称查全率

recall=(TP)/(TP+FN)

#### F-测量与$F_B$测量

$F_1$=$\frac{2\times precision \times recall}{precision + recall}$

$F_B$=$\frac{(1+B^2) \times precision \times recall}{B^2 \times precision +recall}$

